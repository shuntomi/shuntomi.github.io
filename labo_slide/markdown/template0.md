
### Conclution
* 本研究ではGユニットとジェスチャーフェーズ分割のためのアプローチを概説した．
* これを分類問題として扱い，SVMを採用した．
* Gユニットの分割に関して
  * 最良の結果として,条件の複雑度に応じて$0.712〜0.910$まで変化するスコアを達成した．
* ジェスチャーフェーズ分割に関しては，いくつかのSVMモデルを定義した．
* いずれの場合もレストポジションとストークの分類に関して有望な成果を残せた．

---

### ジェスチャー分析領域に対する貢献
1. 人間の行動に依存するデータによる複雑さを分析した事
1. ジェスチャー分析の専門家による結果を使って分析をした事

---

### 人間の行動に依存するデータによる複雑さ
* 分類器は人の振る舞いに強い影響を受ける事が確認できた．
* ただし，ビデオ依存のタスクに関しては非常に良い結果を上げる事ができた．
* これは，アナリストはビデオの前半を手動でラベル付すれば，残りのフレームのラベル付を自動化に行う事を可能にしたからである．
* $30\%$をSVMの訓練に使用し，残りをテストに使用したサブ実験を行った．
  * 結果は精度が$0.812$，リコール$0.907$，F値が$0.857$であった．

---

### セグメントによる分析
* ビデオ依存のタスクにおける最良の分類器が良い結果を残した．
* ジェスチャーフェーズ分割において，ビデオ依存タスクのエラー率が31%を示た．
* このエラーの内67%が遷移フレームに対応してた．
* 提唱モデルと専門家の間の違いの多くが遷移フレームで発生すること示している．

---

### 活用場面と可能性
セグメント別の分析結果とビデオ依存タスクの結果は，

統合的ジェスチャーフェーズ分割に特化したシステムと，

専門家の意思決定をサポートするモジュールを備えた,

注釈ツールの実装を支援できる可能性を示せた．

---

### ジェスチャフェーズ分割自動化の利点
* アノテーションプロセスの効率と精度を高める可能性がある．
* 手動注釈には疲労とフレーム内での動きの視覚分析による不正確さがある．
* 自動化できれば，この両方によって引き起こされる主観的な影響を低減できる．

---

### 問題点1
* 今回はwindowによる時間表現アプローチを採用した．
* この仕組はビデオの最初と最後のフレームのラベルを考慮できない．
* また，最良モデルはラベルを中央に配置していた．
* これは，前後のフレームが分類に必要不可欠な事を意味している．
  * リアルタイムでの使用は遅延が発生する問題点がある．

---

### 問題点2
* 加えてこのアプローチは，準備，ホールド，後退に分割する関しての限界を提示した．
* この限界は，フェーズを速度を利用して分割するアプローチに起因する．
* 手動の場合は視覚情報を利用する．
* 視覚による分析では，時間間隔からホールドとレストポジションを判別する事も可能
* 階層的アプローチは最後に分類するフェーズのエラーが多発する事による，全体エラー率の増加

---

### 改善すべき点
* データ表現の改良
  * 手の位置や手首の角速度
  * tension hand information
* その他のアプローチ
  * 他クラス分類問題のアルゴリズムの導入
  * ジェスチャーの構造を決める規則を定義

---

### 議論すべき要素
* 個別に手の動きを分析する事の妥当性
* 各手の対話の内容との関連，
* インタビュー，ディベート，レクチャーなど異なる状況での分析

---

Thanks!!

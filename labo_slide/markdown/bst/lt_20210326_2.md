# 機械学習についての概論

## 2021/03/26 富澤

---

# 目的

機械学習についての一般的な話をしてみます。

機械学習使おうと思った時の助けになれば嬉しいです。

---

# 自己紹介

開発3部が消滅してしまったため開発2部に移動になりました。

富澤と申します。よろしくお願いします

cash → 外部査定

---

学生時代に人間の非言語情報をコンピューターに認識させる研究をしていて

機械学習とデータサイエンスをかじっていました。

<img src="./image/bst/research.png" width="700px">

---

# 前提

* 機械学習について知らない人向け
* 全体を説明することを重視しています。
* 各手法の話も詳しくは説明しない(時間がないので)

---

### 今日の内容

1. データ分析のこと
1. 機械学習について
1. 評価について (時間があれば)
1. アルゴリズムの例 K-means法 (おまけ)

---

## 1. データ分析のこと

---

### データ分析の流れ

1. 目的設定 & 計画
1. データ設計 & 収集
1. データの前処理
1. <font color='firebrick'>分析手法の選択と適用(機械学習の出番はここ)</font>
1. 実施と検証

このスライドでは4の概要を説明します。

---

### データ分析に使われる知識

- 統計学、機械学習、数理最適化の知識
- プログラミングの知識

---

### よく使われる言語やライブラリ

- Python
  - Numpy, SciPy, Pandas, Matplotlib
  - Sklearn, Keras, Tensor-flow,Pytorch
- R
  - dplyr
  - ggplot
- matlab(有料)

---

## 2. 機械学習について

---

### 機械学習とは

wikipediaよりコピペ
><font color='firebrick'> 機械学習（Machine Learning）</font>とは、人工知能における研究課題の一つで、人間が自然に行っている学習能力と同様の機能をコンピュータで実現しようとする技術・手法のことである。

---

<img src="./image/bst/machine_learning.jpg" width="700px">

---

### 応用分野

機械学習は多くの分野で応用されています。
- 画像解析,文字認識
- 音声解析
- 自然言語処理
- ロボット
- 市場予測
- 医療診断

---

### 機械学習の分類

機械学習の手法は大きく3つに分けられます。
- 教師あり学習
  - 回帰
  - 分類
- 教師なし学習
  - クラスタリング
  - 次元削減
  - 異常検知
- 強化学習(省略)

---

<img src="./image/bst/machine_learning.jpg" width="700px">

---

### その他の分類

他にもいろいろあります。
- アンサンブル学習(XGBoostが有名)
- 半教師あり学習
- 転移学習
- 順序学習
- オンライン学習

---

## 教師あり学習

---

### 教師あり学習とは

<font color='firebrick'>教師あり学習（Supervised Learning）</font>とは  
入力と出力の組からなるデータをもとに学習を行う方法です。

---

### 教師あり学習の分類

教師あり学習は回帰問題と分類問題に分類されます。
- 回帰問題
  - 出力が連続値
  - 関数近似みたいなやつ
  - 例. 明日の株価の終値を予測する
- 分類問題
  - 出力が離散値(1,2,3....)
  - その名の通りグループ分けする
  - 例. 顔画像が本人か判別する

---

### 回帰

<font color='firebrick'>回帰（regression）</font>とはデータに良くあてはまる曲面を見つける分析方法です。

曲面が見つかれば、数値を予測することが可能になります。

<img src="./figures/01/001.png"></img>

---

### 回帰分析の目的

回帰分析は主に２つの目的で利用されます。
- 『過去』から『未来』を予測すること
- 『結果』に対する『原因』を推測すること

---

### 単回帰

例えば単回帰という方法があります。  
データに直線を当てはめる方法です。  
<img src="./figures/01/002.png"></img>

---

### 上手くいかない例

ただし、単回帰ではうまくいかない場合もあります。  
<img src="./figures/01/004.png"></img>


---

### 別の回帰手法

左は重回帰、右は決定木回帰の例です。  
<img src="./figures/01/006.png"></img>
<img src="./figures/01/005.png"></img>  
他にも様々な回帰手法が知られています。

---

### 回帰ができると...

株価の予測とかもできそうです。  
できたらウハウハですね。

<img src="./figures/01/200.png", width='400px'></img>

---

### 分類

<font color='firebrick'>分類（classification）</font>とはデータを隔てる境界線を見つける分析方法です。  
大きく２クラス問題と多クラス問題に分けられます。
<img src="./figures/01/007.png", width='400px'></img>

---

### ２クラス分類

SVMを用いた２クラス分類の例です。  
<img src="./figures/01/008.png", width='400px'></img>

[SVMと双対問題](https://shuntomi.github.io/labo_slide/dual_svm.html)

---

### 多クラス分類

多クラスのデータを分類する問題もあります。  
<img src="./figures/01/009.png", width='400px'></img>

---

### 別の分類手法

左から決定木、N近傍法、多クラスSVMの例です。  
<img src="./figures/01/010.png", width='250px'></img>
<img src="./figures/01/011.png", width='250px'></img>
<img src="./figures/01/012.png", width='250px'></img>

---

### 分類ができると...

手書き文字や物体の認識もできそうです。

<img src="./figures/01/mnist.png"></img>


---

### 教師あり学習の手法

他にもたくさんの手法が知られています。

- K-nearest-neighbor
- 決定木
- SVM
- ニューラルネットワーク(CNN,RNN,LSTM...)
- ランダムフォレスト

---

## 教師なし学習

---

### 教師なし学習とは

<font color='firebrick'>教師なし学習（Unsupervised Learning）</font>とは  
入力と出力の組からなるデータをもとに学習を行う方法です。

---

### 教師なし学習の分類

教師なし学習はクラスタリングと次元削減に分類されます。
- クラスタリング
  - グループわけするやつ
- 次元削減
  - へらすやつ

---

### クラスタリング

<font color='firebrick'>クラスタリング（Clustering）</font>とは  
データにグループを割り当てる手法です。  
<img src="./figures/01/015.png", width='400px'></img>

---

### クラスタリングの例

左がK-means、右がDBSCANの例です。  
<img src="./figures/01/017.png", width='250px'></img>
<img src="./figures/01/016.png", width='250px'></img>

---

### 次元削減

<font color='firebrick'> 次元削減（Dimension Reduction）</font>とは  
高次元のデータを低次元のデータに変換する方法です。  
<img src="./figures/01/013.png", width='250px'></img>

---

### 次元削減の例

左がLLE、右がtSNEの例です。  
<img src="./figures/01/014.png", width='250px'></img>
<img src="./figures/01/018.png", width='250px'></img>

---

## 教師なし学習の手法

他にもたくさんの手法が知られています。

- K-means
- HMM
- トピックモデル
- [主成分分析](https://shuntomi.github.io/labo_slide/pca.html)
- 因子分析
- 独立成分分析

---

### ノーフリーランチの定理

『あらゆる問題で性能の良い  
万能な学習アルゴリズムは存在しない。』

なので、目的に適したアルゴリズムを選択しましょう。

---

## 評価について(時間があれば)

---

### 回帰モデルの評価基準

- 平均絶対誤差
  - $\mathrm{MAE} = \frac{1}{N}(\sum\_{i=1}^{N} |y\_i - \hat{y\_i}|)$
- 平均二乗誤差
  - $\mathrm{MSE} = \frac{1}{N}(\sum\_{i=1}^{N}(y\_i - \hat{y\_i})^2)$
- 平均絶対パーセント誤差
  - $\mathrm{MAPE} = \frac{1}{N}|(y\_t - \hat{y}\_t)/y\_t|$

---

### 分類モデルの評価基準

- 精度
  - 正解数/データ数
- 誤差率
  - 1 - 精度
- 混同行列(ROCカーブ)
  - 適合率
  - 再現率
  - F値

データの隔たりによって適切な評価指標を選ぶ事が大切です。

---

## 4.アルゴリズムの例 K-means法 (おまけ)

---

機械学習のアルゴリズムの一例としてK-means法の  
アルゴリズムを紹介しようと思います。

---

# K-means法を使った分類結果
<img src="./figures/01/017.png"></img>

---

K-means法はクラスタリング手法の一つで、  
おそらく最も有名なクラスタリングアルゴリズムです。

---


$D$次元ユークリッド空間上の$N$個の観測点で構成される  
データ集合$\{ \mathbf{x}\_1,\dots,\mathbf{x}\_N \}$が存在する場合を考えます。  
<br>
K-means法では、データ集合を$K$個のクラスタに分割することを目的とします。  
(※  とりあえず今回は$K$の値は人が与える事にしときます。)

---

直感的にクラスタとは、その内部のデータ点間の距離が、  
外部のデータとの距離と比べて小さいデータのグループのことです。
<img src="./figures/01/017.png"></img>

---


これは代表ベクトルと呼ばれる$K$個の$D$次元ベクトル$\mathbf{\mu}\_k \;\; (k=1,\dots,K)$を  
導入することで定式化することができます。($\mathbf{\mu}\_k$は$K$番目のクラスタの代表ベクトルです)  
<br>
先に言っておくと、$\mathbf{\mu}\_k$は$K$番目のクラスタの中心とみなすことができます。


---


K-means法では、ベクトルの集合$\{ \mathbf{\mu}\_k \}$をうまく決定し、  
全データ点をうまく各クラスタに対応させることで、  
各データ点から対応する$\mathbf{\mu}\_k$への二乗平均の総和の最小化を目指します。

---


ここで、データ点のクラスタへの割当を表す記法を定義しておくと便利です。  
データ$\mathbf{x}\_n$がクラスタ$k$に帰属するか否かを表す変数$r\_{nk}$を以下の様に定義します。

\begin{eqnarray}
r\_{nk} = \begin{cases}
1 & (\mathbf{x}\_nがクラスタkに属する場合) \\\\
0 & (それ以外の場合)
\end{cases}
\end{eqnarray}

---

そして、K-means法の目的関数$J$を次の様に定義します。

\begin{equation}
J = \sum\_{n=1}^{N} \sum\_{k=1}^{K} r\_{nk} \|\| \mathbf{x}\_n - \mathbf{\mu}\_k \|\|^2
\end{equation}

これは各データ点からそれらが割り当てられたベクトル$\mathbf{\mu}\_k$までの  
二乗和の総和を表しており、歪み尺度と呼ばれることもあります。

K-means法の目的は、$J$を最小にする$\{ r\_{nk} \}$と$\{ \mathbf{\mu}\_k \}$を求めることです。


---


$J$の最小化は$\mathbf{\mu}\_k$を初期化した後に、次の2つのステップを繰り返すことで実現できます。
<p></p>
<div style="padding: 20px; margin-bottom: 10px; border: 2px solid #333333;">

<table align="center">
<tr><td>
【ステップ1】: $\mathbf{\mu}\_k$を固定しつつ$r\_{nk}$について$J$を最小化する。<br>
【ステップ2】: $r\_{nk}$を固定しつつ$\mathbf{\mu}\_{nk}$について$J$を最小化する。<br>
</td></tr>
</table>
</div>

---


まず、$r\_{nk}$について$J$を最小化することを考えます。  
$J = \sum\_{n=1}^{N} \sum\_{k=1}^{K} r\_{nk} \|\| \mathbf{x}\_n - \mathbf{\mu}\_k \|\|^2$より、$J$は$r\_{nk}$について線形なので、  
最適化は代数的に解くことが可能です。

異なる$n$を含む項は互いに独立なので、各$n$について別々に、$\|\| \mathbf{x}\_n - \mathbf{\mu}\_k \|\|^2$が  
最小になるような$k$の値に対して$r\_{nk}=1$とすれば良いです。

つまり、次式のように$r\_{nk}$を決めれば良いです。
\begin{eqnarray}
r\_{nk} = \begin{cases}
1 & k=\mathrm{arg} \min\_j \|\| \mathbf{x}\_n - \mathbf{\mu}\_j \|\|^2のとき \\\\
0 & (それ以外の場合)
\end{cases}
\end{eqnarray}

---


次に$\mathbf{\mu}\_k$について$J$を最小化することを考えます。  
$J = \sum\_{n=1}^{N} \sum\_{k=1}^{K} r\_{nk} \|\| \mathbf{x}\_n - \mathbf{\mu}\_k \|\|^2$より、$J$は$\mathbf{\mu}\_k$の二次関数なので、  
次のように$\mathbf{\mu}\_k$に関する偏微分を$0$と置くことで最小化できます。
\begin{equation}
2 \sum\_{n=1}^N r\_{nk}(\mathbf{x}\_n - \mathbf{\mu}\_k) = 0
\end{equation}


---

$\mathbf{\mu}\_k$について解くと、次式を得ます。  
\begin{equation}
\mathbf{\mu}\_k = \frac{\sum\_n r\_{nk} \mathbf{x}\_n}{\sum\_n r\_{nk}}
\end{equation}
<br>
この式は、$\mathbf{\mu}\_k$を$k$番目のクラスタに割り当てられたすべてのデータ点$\mathbf{x}\_n$の  
平均値とおいていると解釈することができます。(これがK-means法の名の由来です)

---

K-means法のアルゴリズムをまとめると次のようになります。
<p></p>
<div style="padding: 20px; margin-bottom: 10px; border: 2px solid #333333;">

<table align="center">
<tr><td>
1. $\mathbf{\mu}\_k$の初期値を選ぶ<br>
<p></p>
2.【ステップ1】以下の式で$r\_{nk}$を計算。<br>
\begin{eqnarray}
r\_{nk} = \begin{cases}
        1 & k=\mathrm{arg} \min\_j \|\| \mathbf{x}\_n - \mathbf{\mu}\_j \|\|^2のとき \\\\
        0 & (それ以外の場合)
        \end{cases}
\end{eqnarray}
<p></p>
3.【ステップ2】求めた$r\_{nk}$で$\mathbf{\mu}\_{nk}$を再計算。<br>
\begin{equation}
\mathbf{\mu}\_k = \frac{\sum\_n r\_{nk} \mathbf{x}\_n}{\sum\_n r\_{nk}}
\end{equation}

4. 収束条件が満たされていなければ、ステップ2に戻る。
</td></tr>
</table>
</div>

上記のように2つのステップを収束するまで（もしくはあらかじめ定めた  
最大繰り返し数を超えるまで）繰り返します。  
<a href="http://tech.nitoyon.com/ja/blog/2013/11/07/k-means/" target="blank">K-meansのアニメーション</a>

---

以上です。ありがとうございました。

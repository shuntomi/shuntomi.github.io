# Gesture phase segmentation using support vector machines
### 2018/10/25 富澤駿

---

## 参考文献
##### Madeo, Renata Cristina Barros, Sarajane Marques Peres, and Clodoaldo Aparecido de Moraes Lima.<br> "Gesture phase segmentation using support vector machines." Expert Systems with Applications 56 (2016): 100-115.

---

## 目次
1. <font color="Red">Abstract</font>
1. Introduction
1. 理論的背景
1. ジェスチャ分割の自動化
1. 人間の振る舞いへの依存
1. An alternative analysis
1. Conclution

---

### Abstract

* 人間のコミュニケーションにはジェスチャーが多く含まれいる．
* ジェスチャーによるサポートがあれば，人間と機械のコミュニケーションはより円滑になる可能性がある．
* ジェスチャーの解析を行う時にはいくつかの問題を考慮する必要がある．
* そのうちの一つがジェスチャーフェーズ分割に関する問題である．
* ジェスチャーは"レストポジション","準備","ストローク","後退"から構成される
* 会話中のジェスチャーの意味解析などを行う場合，まずジェスチャーを分割する必要がある．

---

### 本論文の目的
* ジェスチャー分割の自動化を行う事
* ジェスチャー分割問題を多クラス分類問題として定式化する．
* 具体的には，SVMを使用する
* 各フェーズからジェスチャの固有パターンを学習するモデルを設計する事を目指す．

---

### 本研究の特徴は大きく分けて二点
1. セグメンテーションアプローチの限界に取り組んだ点
  * 人間の行動パターンを分析する複雑さを表すさまざまなシナリオでのパフォーマンスの研究を通じて，セグメンテーションアプローチの限界に取り組んだ
  * 分類モデルが人間の振る舞いにどのように影響を受けるのを調べた．
2. 本研究は言語学や心理学の理論を考慮した点

---

## 目次
1. Abstract
1. <font color="Red">Introduction</font>
1. 理論的背景
1. ジェスチャ分割の自動化
1. 人間の振る舞いへの依存
1. An alternative analysis
1. Conclution

---

### Introduction
* ジェスチャー分析に関する研究が盛んである
* ジェスチャー分析はいくつかの種類がある
  * 身体の一部を使ったジェスチャー分析
  * 複数ユーザーが同時に一つまたは複数のデバイスと行う対話の分析
* ジェスチャー分析への関心を高めた要因の一つは，センサーの発展
  * 人間の動作をさまざまなレベルで詳細にサンプリングする事が可能になった．
* 本論文の目的は，**ジェスチャー分割の自動化を行う事**

---

### 論文の構成
* まず3章で，ジェスチャ分割とSVMの理論的な説明を行う
* 次に4章で，ジェスチャ分割の自動化のアプローチ，実験設定，実験結果とその考察を行う
* 次に5章で，データの収集とSVMの訓練を考慮した場合，人間の行動分析がどのくらい複雑なのかを考察する
* 次に6章で本論文の分析が，ジェスチャー分析の分野で行われている物と似ている事を示すために，いくつかの立場を議論する
* 最後に7章で，本論文の結論を述べる

---

## 目次
1. Abstract
1. Introduction
1. <font color="Red">理論的背景</font>
1. ジェスチャ分割の自動化
1. 人間の振る舞いへの依存
1. An alternative analysis
1. Conclution

---

### 3. 理論的背景
1. <font color="Red">ジェスチャーフェーズ分割に関する理論</font>
1. SVMに関する理論

---

### ジェスチャーフェーズ分割の理論
* ジェスチャーは統合的な構造を持つ
* ジェスターの1つの単位を<font color="Red">ジェスチャーユニット(G-unit)</font>と呼ぶ．
  * 腕がレストポジションから離れる時が開始位置
  * レストポジションに戻る時が終了位置
* このG-Unitはさらに<font color="Red">ジェスチャーフェーズ</font>に分解する事が可能

---

### ジェスチャーフェーズの詳細
1. 準備: 手がジェスチャの開始位置まで移動
2. プレストローク: 準備の終了と同時に，手の形と位置を維持したまま一時停止
3. ストローク: ジェスチャーの意味を表す動き
4. ポストストローク: ジェスチャの終了と同時に，手の形と位置を維持したまま一時停止
5. 後退: 手が休息位置に戻る

---

<img src="./image/paper01/fig01.jpg" width="500"></img>
### G-unitとジェスチャーフェーズの例

---

### G-unitについて
* Gユニットは$n$個のフェーズから構成される
* $n$の値は$n=1,\dots,\infty$のいずれかである．
* ただし，各フェーズは表現に幅を持っている．
* つまり，先に述べた構成が保証されない時がある．
  * 一部のフェーズが先行する．
  * 欠けている場合もある．

---

### 表現フェーズ

* その意味を含んでいるジェスチャーのセグメントを特定するフェーズ
* どのフェーズが当てはまるかは一般には分からない
* これらは以下の要素で構成される
	* 独立したホールド
	* 任意のプレストロークホールド
	* 義務ストロークホールド
	* 任意のポストストーロークホールド

---

* 特にプレストロークホールドとポストストロークホールドは**従属ホールド**と呼ばれる．
* 独立ホールドは新たに定義された概念
  * ジェスチャーが一時停止で表現され，関連するストロークが無い場合に発生．

---

<img src="./image/paper01/fig02.jpg" width="400"></img>

「あなた」を表すジェスチャーから抽出されたフレーム

"独立ホールド"と２つの"レストポシション"で構成

---

### ジェスチャーフェーズ分割の問題点
* 通常の場合，ジェスチャーフェーズの分割は専門家によってラベル付けが行われる．
* しかしこれは以下の４つの点で困難である．
    1. 主観的なタスクになってしまう点である．　
    2. 同じように振る舞うフェーズが存在する点
    3. ジェスチャーフェーズ分割は，ジェスチャー研究界でも未解決問題が残っている点
    4. 手動のジェスチャーフェーズ分割は，とても時間が掛かる作業である点．

---

### 1. 主観的なタスクになってしまう点
* どのフレームがフェーズ間の移り変わりを示しているかは非常に分かりづらい
* このため，同じビデオに対して多くの不一致が見られる場合がある．
* 専門家の分析結果を使用する場合，不一致を図る指標を導入する必要がある．

---

### 2. 同じように振る舞うフェーズが存在する
* 例えば，レストポジションとホールドは期間は手が停止している点では同じ
* 準備と後退は始点と終点を入れ替えればまったく同じ動作である．

---

### 3. 未解決問題が残っている点
* 具体的を上げてみる
    * 手を個別に解析する必要があるかどうか
    * 休止位置の変化を特定の特徴やジェスチャーと見なすかどうか
    * 意味を持たないジェスチャーを分析の時に考慮すべきかどうか
* これらの立場は専門家によっても様々である
* この立場は結果に影響を与える可能性がある

---

### 4. 時間が掛かる作業である点
* 1分の解析に10日必要な場合もある
* この点でも作業の一部でも自動化ツールは非常に役立つ可能性がある．

---

### 3. 理論的背景
1. ジェスチャーフェーズ分割に関する理論
1. <font color="Red">SVMに関する理論</font>

---

### SVM

* SVMは汎化性能を最大化させる事を目指した教師あり学習アルゴリズム．
* 訓練データ:  $ \left\\{ \boldsymbol{u\_{i}},y\_i \right\\}^{N}\_{i=1}$に対する二値分類問題を考える.
* $N$: サンプルデータ数
* $\boldsymbol{u_i} \in \mathbb{R}^R$: $R$次元の入力ベクトル
* $y_{i} \in \left\\{ \pm 1 \right\\}$: 教師ラベル

---

SVMの目的は超平面によってクラスを分離する事である．
以下に式を示す．

\begin{equation}
	f\left( \boldsymbol{u\_i}\right) =
  \left< \boldsymbol{w\_o} \cdot \varphi \left(\boldsymbol{u\_i}\right) \right> + b\_{0}
\end{equation}

* $\boldsymbol{w\_o} \in \mathbb{R}^N$: 重みベクトル
* $b_0$: バイアス
* $\varphi$: 入力ベクトルの写像．
* $\left<\cdot \right>$: 内積を表す演算子

---

一般に，２クラスを分離できる超平面は無数に存在する可能性があるが，

超平面と各クラス内の最も超平面と

近いサンプルとの距離を最大化するという条件の元では，

超平面は唯一つに定まる．

---

### SVMは最適化問題として定式化できる．
\begin{align}
    \min\_{\boldsymbol{w}, b, \boldsymbol{\xi}}
    & & &
    \phi\left( \boldsymbol{w},b,\boldsymbol{\xi} \right) = \frac{1}{2}||\boldsymbol{w}||^2 + C \sum\_{i\in[n]} \xi\_i
    \\\\
    \\\\
    \mathrm{s.t.}
    & & &
    y\_i(\boldsymbol{w}^{\mathrm{T}}\boldsymbol{x}\_i + b) \geq 1 - \xi\_i
    \quad
    i \in [1,\dots, N]
    \\\\
    & & &
    \xi\_i \geq 0
    \quad
    i \in [1, \dots, N].
\end{align}
* バラメータ$C$はペナルティの強さを表す値である．

---

$\xi\_{i}$は，制約を満たすために調整されるパラメータであり，

誤判別を許容する代わりに課されるパラメータといって良い.

つまり，$\xi\_i$の取る値によって，全てのデータに対して制約を満たす事ができる．

\begin{eqnarray}
\begin{cases}
	\xi\_{i} = 0 & \left( 判別成功 \right) \\\\
	0 < \xi\_{i} \leq 1 & \left( 成功だがマージンを超える \right) \\\\
	\xi_{i} > 1 & \left( 誤判別 \right)
\end{cases}
\end{eqnarray}

---


双対問題への変形は元論文になかったので，省略しましたが，

僕が昔作った資料があるのでよかったら参考にしてください．

<h4><a href="https://shuntomi.github.io/labo_slide/dual_svm.html">リンク</a></h4>

---

最適化問題として記述できた事から，

ラグランジュの未定乗数法とKKT条件から双対問題を導く事ができる．

以下に式を示す．

\begin{align}
    \min\_{\boldsymbol{w}, b, \boldsymbol{\xi}}
    & & &
    L\_1\left( \boldsymbol{\alpha} \right) = \sum\_{i=1}^{N}{\alpha\_{i}} - \frac{1}{2}\sum\_{i=1}^{N}\sum\_{j=1}^{N}{\alpha\_{i}\alpha\_{j}y\_{i}y\_{j}  \boldsymbol{\varphi^{T} \left(x\_i\right)}\boldsymbol{\varphi \left(x\_j\right)}}
    \\\\
    \\\\
    \mathrm{s.t.}
    & & &
    \sum\_{i=1}^{N}{\alpha\_{i}y\_{i}} = 0
    \\\\
    & & &
    0 \leq \alpha\_{i} \leq C
    \quad
    i \in [1, \dots, N].
\end{align}

ここで，$\alpha$はラグランジュ未定乗数である．

---

ここまで，$\varphi$の中身を明示していないが，

双対問題の式を見ると，その内積$\varphi \left(\boldsymbol{u\_{j}}\right) \cdot \varphi\left(\boldsymbol{u\_{i}}\right)$の形を定めれば，良い事が分かる．

これをカーネル関数と呼び$K\left(\boldsymbol{u\_{j}},\boldsymbol{u\_{i}}\right)=\varphi \left(\boldsymbol{u\_{j}}\right) \cdot \varphi\left(\boldsymbol{u\_{i}}\right)$で表す．

---

### カーネルトリック
* カーネル関数を使った方法は，分離可能になる高次元空間に訓練データを射影する事
* その射影関数$\varphi$は明示する必要はない
* その内積$K\left(\boldsymbol{u\_j}, \boldsymbol{u\_i}\right)$のみを定めれば良い
  * これによって次元の呪いの影響を最小限に抑える事ができる．

<h4><a href="https://www.youtube.com/watch?v=3liCbRZPrZA">参考動画</a></h4>

---

無限次元への射影に対応するカーネル関数である

RBFカーネルは以下の式で表される．

\begin{equation}
	K\left(\boldsymbol{u\_{i}},\boldsymbol{u\_{j}}\right)=\exp\left( \frac{\| \boldsymbol{u\_{j}}-\boldsymbol{u\_{i}}\|}{2\sigma^{2}} \right)
\end{equation}

$\sigma$はハイパーパラメータを表す．

---

## 目次
1. Abstract
1. Introduction
1. 理論的背景
1. <font color="Red">ジェスチャ分割の自動化</font>
1. 人間の振る舞いへの依存
1. An alternative analysis
1. Conclution

---

### ジェスチャ分割の自動化
* 元々の目的は，ジェスチャーフェーズ分割の自動化のためのアプローチを作る事
* 分類タスクとして問題をモデル化する事でこの目的の達成を試みる

---

### 4. ジェスチャ分割の自動化
1. <font color="Red">データの説明と前処理</font>
1. 問題定義
1. 実験設定
  1. Gユニット分割とパラメータチューニング
  1. Gユニットからジェスチャーフェーズ分割

---

### データセット
* データセットは自然な会話から構成された物である必要がある．
* 今回はストーリを説明するタスクを選択した
  * 理由は，ジェスチャーを自然に誘発しやすいタスクだという研究報告があるため
* 被験者は３つの漫画を提示され，その内容についてKinectに対して説明を行う

---

#### 各ビデオに関する情報
<img src="./image/paper01/tabel1.png" width="600"></img>

---

### データの収拾
* Kinectを使用
* 各フレームに対応する画像と手，手首，頭，脊髄の座標を取得
* 各画像は専門家三人によってラベル付けされている．
* 3人である理由は，ジェスチャフェーズ分割は主観的なタスクであるから
* この問題に対処するため，三人の評価を元にクリッペンドルフの$\alpha$係数を求める．
  * この値が$1$に近いほど3人の一致率が高い

---

### 前処理
* 手と手首に関しては，頭と脊髄の座標を元に正規化を適応した．
  * カメラに対して水平方向とユーザ対カメラ間で不変である位置を求めるため
* 正規化したデータから，速度と加速度をベクトルとスカラーの2種類として求めた．
  * これらを求めるためのフレーム変位は$1,3,5$の三種類を使用した．

---

### 4. ジェスチャ分割の自動化
1. データの説明と前処理
1. <font color="Red">問題定義</font>
1. 実験設定
  1. Gユニット分割とパラメータチューニング
  1. Gユニットからジェスチャーフェーズ分割

---

### 問題定義
本論文では，ジェスチャー分析を二値分類問題として定式化する．

すなわち，入力空間$\mathbb{R}^R$とした時，

\begin{equation}
	\mathbb{F}: \mathbb{R}^R \times \phi\left(\boldsymbol{u}\right) \to y
\end{equation}

で表す写像をモデル化する事を目指す．

* $\boldsymbol{u\_{i}} \in \mathbb{R}^R$: ジェスチャ内の関心のあるフレームを表す$R$個の属性からなるベクトル
* $y=\{ \pm1\}$: 対象のフレームが特定のユニットやフェーズに含まれているかを表すラベル

---

#### 属性数$R$の決定
* 本研究では各フレームが32までの属性を持つベクトルとして記述できる．
* 左右の手と手首に対して，以下の値を使用できる
  * ベクトル速度(3)
  * ベクトル加速度(3)
  * スカラー速度(1)
  * スカラー加速度(1)
* 最終的にはどの属性を使うか決定する必要がある

---

#### windowedアプローチ
* 各フェーズの固有パターンを調べるために使用する
* 時系列的な関連を，データ表現に含める事ができる．
* windowedアプローチのフレームの表現は$m$個の単純な長方形で構成される．

---

* $m$は$1\sim 80$のwindowサイズを表す．
* windowの構成は興味対象のフレームが，window内のどこのポジションになるかによって，以下の2つに分かれる．
	* フレームがwindowの末尾に追加される場合．この場合，過去の情報のみが考慮される．
	* フレームがwindowの中央に追加される場合．過去と未来の情報が考慮される．

---

つまりwindowを$w$で表せば，

\begin{equation}
	w = \left[ f\_1,\dots,f\_{\left[ m/2\right]},\dots,f\_m \right]
\end{equation}

で定式化される．なお$f\_{\left[ m/2\right]},f\_m$はテストされたケースで関心のあるフレームである．

---

<img src="./image/paper01/fig03.jpg" width="500"></img>
#### Windowedアプローチ

---

### 4. ジェスチャ分割の自動化
1. データの説明と前処理
1. 問題定義
1. <font color="Red">実験設定</font>
  1. <font color="Red">Gユニット分割とパラメータチューニング</font>
  1. Gユニットからジェスチャーフェーズ分割

---

### 準備

SVMの正則化パラメータを決定するため，予備実験を行った．

$C$は$C=\{1,10,100,1000\}$から安定した結果を残すパラメータを選択する事にした．

その結果$C=100$が最も安定した結果を残した．


また，これ以降の実験では，

SVMの訓練はユーザーAがストーリー1を解説したビデオを使用し，

テストはユーザーAのストーリー2に関するビデオを使用した．

>>>

### SVMは最適化問題として定式化できる．
\begin{align}
    \min\_{\boldsymbol{w}, b, \boldsymbol{\xi}}
    & & &
    \phi\left( \boldsymbol{w},b,\boldsymbol{\xi} \right) = \frac{1}{2}||\boldsymbol{w}||^2 + C \sum\_{i\in[n]} \xi\_i
    \\\\
    \\\\
    \mathrm{s.t.}
    & & &
    y\_i(\boldsymbol{w}^{\mathrm{T}}\boldsymbol{x}\_i + b) \geq 1 - \xi\_i
    \quad
    i \in [1,\dots, N]
    \\\\
    & & &
    \xi\_i \geq 0
    \quad
    i \in [1, \dots, N].
\end{align}
* バラメータ$C$はペナルティの強さを表す値である．

---

### Gユニット分割
* 初めの実験の目的は以下の2つ．
  1. 次の実験のために，残ったパラメータの値を定義する事．
  2. SVMとGユニット分割に関するwindowアプローチのパフォーマンスを解析する事
* 実験は3つのステップで構成される

---

話を整理するため，この実験で決定するパラメータを示す．
* 訓練データとテストデータのバランス
* 速度決定のためフレームの変位(1 or 3 or 5)
* 特徴のタイプ(スカラーorベクトルor両方)
* 特徴の種類(加速度or速度or両方)
* 使用する部位(手or手首or両方)
* Window内でのポシション(真ん中or最後)

---

### 準備
予め以下のパラメータを動かし，SVM集合を作成しておく
* $\sigma=\{ 2^{-2},2^{-1},\dots,2^{4} \}$
* windowsサイズ: $=\{ 1,2,\dots, 80\}$

---

### 今は2クラス問題
* 入力: windowサイズ分のフレーム
* 出力: 自分がユニットに入ってるかどうか

---

# First step
まず，フレームの変位，特徴のタイプ，訓練データセットのバランスを決める

1. windowの中央でラベルの位置を一旦固定する．
1. \{手，手首，両方\}と\{速度，加速度，両方\}の全組み合わせにたいしてそれぞれSVMを構築する．
1. 得られたSVMモデルのセットから，パラメータの各組み合わせについて**最良**のFスコアを有するものを選択した．

---

#### 変位，特徴量の種類，訓練データのバランスのサーチ結果
<img src="./image/paper01/table2.png" width="600"></img>

選択後の平均の結果

---

### First Stepの結果とパラメータの決定
結果を見ると$Displacement=1$の場合を除き，ほとんど同様な値を示した．

したがって，パラメータ選択は最良の$F=0.861$を示した

$d=3$，特徴量をスカラー，訓練データが不均質な場合を選択した．

---

* <font color="Red">訓練データとテストデータのバランス(アンバランス)</font>
* <font color="Red">速度決定のためフレームの変位(3)</font>
* <font color="Red">特徴のタイプ(スカラー)</font>
* 特徴の種類(加速度or速度or両方)
* 使用する部位(手or手首or両方)
* Window内でのポシション(真ん中or最後)

---

### Second step
目的は使用する部位とその特徴パラメータを測定する事

1. ラベルはwindowの中央で固定する．
1. 変位，特徴量のタイプ，訓練データのバランスは第一段階の結果で固定する．

---

#### 特徴の種類，使用する部位のサーチ結果
<img src="./image/paper01/table3.png" width="600"></img>

各モデルの$F$値の平均の結果を示す．

---

この実験では加速度特徴量以外の結果はほとんど同じであった．

従って，最良の結果$\left( F=0.877 \right)$が得られた，

手と手首を注目点，速度+加速度を特徴量とした場合を選択した．

---

* <font color="Red">訓練データとテストデータのバランス(アンバランス)</font>
* <font color="Red">速度決定のためフレームの変位(3)</font>
* <font color="Red">特徴のタイプ(スカラー)</font>
* <font color="Blue">特徴の種類(加速度+速度)</font>
* <font color="Blue">使用する部位(手+手首)</font>
* Window内でのポシション(真ん中or最後)

---

### Final step
window内のラベルの位置を決定する．
1. 他のパラメータはこれまでに決定したものに固定する．
1. ラベルの位置をWindowの中央から最後に変更した

この場合，得られる最良のスコアは$\left( F=0.746\right)$であった．

---

これはSecond Stepの$F=0.877$から悪化している．

つまりwindowの中央でラベルを固定する方が良いという事が分かった．

言い換えればこの実験においては，

過去と未来のフレームを考慮する必要があるという結果が得られた．

---

話をまとめると，決定されたパラーメータは以下のようになった．
* <font color="Red">訓練データとテストデータのバランス(アンバランス)</font>
* <font color="Red">速度決定のためフレームの変位(3)</font>
* <font color="Red">特徴のタイプ(スカラー)</font>
* <font color="Blue">特徴の種類(加速度+速度)</font>
* <font color="Blue">使用する部位(手+手首)</font>
* <font color="Green">Window内でのポシション(真ん中)</font>

---

#### 最良モデルについての詳細
得られた最良のパラーメータを使用したSVMは,

Windowサイズ$=81$，RBFは$\sigma=16$で得られた．

その精度は，$F=0.877,precision=90.4\%,90.4\%,error=9.8\%$であった．

---

### 4. ジェスチャ分割の自動化
1. データの説明と前処理
1. 問題定義
1. 実験設定
  1. Gユニット分割とパラメータチューニング
  1. <font color="Red">Gユニットからジェスチャーフェーズ分割</font>

---

### ジェスチャーフェーズ分割
* ジェスチャフェーズ分割のためのに2クラス分類モデルを構築する
* しかし行いたい問題は多クラス問題
* 従って2クラス分類問題を組み合わせて行う事になる．
* 採用したアプローチは以下の3つ
  * 1:1アプローチ
  * 1:ALLアプローチ
  * 階層的クラスタリングの3つを採用した．

---

### 今回は5クラス分類問題
1. レストポジション: RP
1. 準備: P
3. ストローク: S
4. ホールド: H
5. 後退: P

---

初めの2つは1対1の2値分類器を複数個使うスタンダードな方法をとる．

3つめは，階層的な構造を持つ分類器を使用する．

この分類器はジェスチャーの専門知識によってヒューリスティック的に構築される．

---

まず全てのアプローチにおいて，

$C=100,\sigma = \left( 2^{-2} ,\dots, 2^{6} \right)$,

$1〜80$のフレームを含むwindowを使用するSVM集合を構築した．

---

#### 注意すべき点
* 「1:1アプローチ」，「1:ALLアプローチ」
  * 分類器の各組みが同じカーネルパラメータと同じwindowサイズを持つ．
* 階層的クラスタリング
  * 異なるカーネルパラメータと異なるwindowサイズを持つ事が可能

---

### 1:1アプローチと1:ALLアプローチ
* 1:1アプローチ
  * 2クラス分類器は各クラスの可能なペアごとに作られる．
  * 最終的は多数決によって分類されるクラスが決定
* 1:ALLアプローチ
  * クラス数$C$個分の2クラス分類器を使用する
  * 最大値を出力したクラスに分類する．

---

#### 1:1と1:ALLのパフォーマンス
<img src="./image/paper01/table4.png" width="400"></img>

---

### 考察
* 両方で似たような結果が得られている事が分かる．
* 分類エラーの最良値は1:ALLの場合で，エラー率は$31.5\%$であった．
  * このSVMのパラメータは$\sigma=16$,winodwのフレーム数は$69$であった．
* エラーに関しての結果は，レストポジションとストローク検出において最良の性能を示している．
  * レストポジションが動きが無い事によって特徴付けられている．
  * ストロークが動きのピークに対応する事が理由だと考えられる．
* レストポジションとホールドとの間に類似性がある
  * 多数のホル-ドがレストポジションとして分類される理由だと考えられる．

---

### 階層構造によるアプローチ
* ジェスチャーフェーズ分割を単純な分類問題に分割する．
* まず，簡単に区別できる度合いに応じて，クラスを並べ替える．
  * 並べ替えるための指標はジュスチャー理論に基づいて決める
* 並べ替えたクラスは以下
  1. レストポジション
  1. ホールド
  1. ストローク
  1. 準備
  1. 後退

---

## 階層的アプローチによるアルゴリズム-1
<br>
##### <font size="6"> 1. レストポジション検出：入力: $f\_{i}\in S$，出力: $c\_{i}=\{ RP,U \}$</font>
  * $U\supset \{P、S、H、R\}$であり，ジェスチャユニットの概念に対応する。

##### <font size="6"> 2. ホールド検出：入力$f\_{i}\in S\_{U}$, 出力: $c\_i=\{H,M\}$</font>
  * $S\_U$は，前の分類器によって$S$に分類されたフレームのサブセット
  * $M \supset \{P、S、 R\}$であり，多くの動きを伴うフェーズに対応する。

---

## 階層的アプローチによるアルゴリズム-2
<br>
##### <font size="6">3. ストローク検出：入力: $f\_{i}\in S\_M$，出力: $c\_i=\{S,T\}$</font>
  * $S\_M$は前の分類器によってMとして分類された$S\_M$のサブセット
  * $T\supset\{P、R \}$はジェスチャユニットの残りのフェーズに対応する。

##### <font size="6">4. 準備/後退分類：入力: $f\_i=\in S\_T$, 出力: $c\_i=\{S,T\}$</font>
  * $S\_{T}$前の分類器によってTと分類されたSMのサブセットである.

---

### 注意点
* このアプローチでは，前の分類で分類されたデータは次の分類では除外される．
* つまり初めの分類器の性能が精度に対して強い影響を持っている事を意味する．

---

次のスライドに結果を示す．なお，

* (a)は実際に入力としてあたえたデータ使って評価した．
* (b)はビデオ全体を評価の対象とし，各フェーズが前の分類器のエラーによって受ける影響を考慮した

---

#### 各分類機ごとのパフォーマンス
<img src="./image/paper01/table5.png" width="600"></img>

>>>

#### 1:1と1:ALLのパフォーマンス
<img src="./image/paper01/table4.png" width="400"></img>

---

### 階層的アプローチの構造的問題
* 前の分類器が次の分類器に悪い影響を与える可能性がある
* この状況は，以下が考えられる．
1. 1つめの分類器が，レストポジションを残りのフレームとした場合．
	* ホールドとレストポジションは特徴が似ている
  * 次のホールド検出器はホールドをレストポジションとして分類し，エラーを引き起こす．
2. 1つめの分類器が，ホールドをレスとポジションとして分類した場合．
	* この場合は，次のホールド検出器は十分なホールドデータが提示されず，性能が著しく落ちる．

---

### 階層的アプローチの考察1
* 階層的アプローチは，全体的なエラーを$28.3 ％$に減らした
* 特に，レストポジションとストローク検出の性能を上げた
* レストポジションの検出は，他の戦略よりも良好であった
  * G-Unit分割実験で得られた結果と同等であった.
* ストロークの検出は，レストポジションの結果が良かった影響を受け，良い結果になっている．
* レストポジション検出器による良好な結果を利用できた事により，良い結果をもたらした．

---

### 階層的アプローチの考察2
* 準備と退去の分析を通じて、階層的戦略の影響を可視化できた．
* $99$フレームの準備が既にレストポジションおよびストロークとして誤って分類されている.
* つまり，準備検出器は，準備のすべてを分類出来ているわけではない．
* これは言い換えれば，後退クラスは他の分類器からすべての誤差の影響を受け，精度を損なってしまうと言える．
* 理由は，後退は最後の分類器で準備として分類されないフレームとして定義されるからである．

---

### 階層的アプローチの考察3
* 階層的アプローチでは，異なるWindowサイズが各分類器に適応されている．
* ビデオを構成する各クラスの平均サイズと，最良の結果を出した分類器によって採用されたWindowサイズには何かしらの関係がある．
* 最良の分類器のwindowサイズは以下
  * 静止位置81フレーム
  * ホールド17フレーム
  * ストローク20フレーム
  * 準備と後退23フレーム
* RBFカーネルパラメータは，順に$\sigma=16,8,4,8$であった．

---

## 目次
1. Abstract
1. Introduction
1. 理論的背景
1. ジェスチャ分割の自動化
1. <font color="Red">人間の振る舞いへの依存</font>
1. An alternative analysis
1. Conclution

---

### 個人の振る舞いへの依存性
* 4章では，同じユーザーが行った2つのストーリーに対するデータを使用した．
* つまり，この実験はユーザーとセッションに依存してる
* ビデオとその内容にも依存していると言える．
* 本来はジェスチャーパターンにバリエーションがあるべきである．
  * これは，人の特性や気分はかなり人の振る舞いに影響を与える事が理由である．

---

### 5章の目的
* モデルのパフォーマンスが個人への依存性に対してどのような影響を受けるかを議論する．
* 特に以下の3つの観点で実験し評価する.
  * (1) セッションによる影響
  * (2) ユーザーの違いによる影響
  * (3) ユーザーが行動パターンを維持できるか

---

### 1. セッションによる影響
* 異なる日に得られたデータが分類器の性能に影響を与えるかどうかを調べる．
* ユーザーAが初日にストーリー1を，2日目にストーリー3説明したデータを使用する
*	ビデオA1のフレームをトレーニングに使用し，ビデオA3のフレームで分類器をテストした
  * **さっきまではA1で学習，A2でテスト**

---

### 2. ユーザーの違いによる影響
* 異なるユーザーが同じ話をした場合を含む．
* ユーザーAとBの両方がストーリー1を話していたビデオを使用する
* A1をトレーニングに使用し，ビデオB1をテストに使用

---

### 3. ユーザーが行動パターンを維持できるか
* 分析は同じビデオで実行され、ユーザーAはストーリー1を説明する．
* ビデオのフレームの最初の70％はトレーニングに使用され、残りの30％はテストに使用される．
* ホールド検出器はビデオの最後の30％以内にホルードフレームが無いため除外した．

---

### 3つの観点でのG-unit分割の結果
<img src="./image/paper01/table6.png" width="500">

---

* 分類器が上の観点での変化の影響を受けることを確認できる．
* ジェスチャーユニットの分割は，観点3において非常に高い精度で達成できた．
  * さっきの最良のスコアは$\left( F=0.746\right)$
* 観点1では異なるセッションの影響が確認され，
* 観点2では異なるユーザーの影響が確認された．
* 特に観点2に関して，各ユニットのフレーム数が不均衡である(下参照)
* 一般的な分類誤差が偏る可能性があるので、Fスコア測定に注意を払うべきである．

>>>

### 各ビデオに関する情報
<img src="./image/paper01/tabel1.png" width="600"></img>

---

### ジェスチャーフェーズ分割に関しての結果
<img src="./image/paper01/table7.png" width="400">

>>>

#### 各分類機ごとのパフォーマンス
<img src="./image/paper01/table5.png" width="600"></img>

---

### 分析結果
* 単一のビデオを用いた分析は，最良のモデルが21.4％の一般的な分類誤差を得た
* データ収拾方法と個人特性による影響が確認できた．

---

### ジェスチャーフェーズ分割に関する考察
* 異なるセッションにおけるユーザのジェスチャ行動の差をはっきりと示した．
  * これは，観点1における間違った分類が原因である．
* また，以下を含むジェスチャーに類似性が確認できた．
  * ストロークおよびレストポジション（観点1）
  * ホールドとレストポジション（観点1と観点2）
* これらの問題の原因を説明するための例を次のスライドに示す．

---

<img src="./image/paper01/fig04.jpg" width="550">
#### ジェスチャーの速度変動による特徴

---

* (a): 第1セッション中のユーザA
  * このセッションのストロークフェーズは，動きの速度のより大きな変化によって特徴づけられている．
* (b): 第2セッション中のユーザA
  * ビデオのいくつかの部分で，同じユーザーのジェスチャーパターンに変化が少なくなっている．
  * いくつかの短い区間において，ゼロである速度パターンを有していた．
  * この現象の例第1ストローク段階で観測できる．
* (c): 第2セッション中のユーザA
  * ホールド位置とレストポジションの類似点を示す部分がある．
  * これらのフェーズの主な違いは，構成されているフレームの数である．

---

## 目次
1. Abstract
1. Introduction
1. 理論的背景
1. ジェスチャ分割の自動化
1. 人間の振る舞いへの依存
1. <font color="Red">その他の方法による分析</font>
1. Conclution

---

### その他の方法による分析
* これまで，機械学習の評価指標を使用した．
* 本研究が主観的性質を持つことを考えると，異なるの分析も必要である．
* この章では以下の2つの方法で分析を行う
  * フェーズ間遷移と内部エラーによる分析
  * セグメントによる分析

---

### フェーズ間遷移エラーと内部エラー
* フェーズ間の遷移を表すフレームは明確では無く，専門家でも意見が分かれる
* 次スライドの図に場合，フレームg〜jがレストポジションの開始点になる可能性が高い．
* 遷移エラーと内部エラーの割合を調べる事によって分析を試みる

---

<img src="./image/paper01/fig05.jpg" width="450">
#### フレーズ間遷移（レストポジションから準備フェーズ）

---

* 遷移エラー
  * フェーズ両端のフレームを隣接するフレームに属する物として分類した場合
  * エッジの変位であり，妥当な誤差と見なせる
* 内部エラー: 他の誤分類

---

#### フェーズ間遷移エラーと内部エラー
<img src="./image/paper01/table8.png" width="500">

---

### 観点1の実験における内部エラー
* レストポジションとして誤分類された321個のフレームのうち，96個がホールドに対応する内部エラーである．
* G-unitに属すると分類されたレストポジションの2つの非常に小さなセグメントに対応する22個のフレームがあった．
  * サイズ44のwindowによって隠されてしまったという考え方もできる．

---

### 観点2の場合
* レストポジションと誤分類された106個のフレームのうち，71個がホールドである内部エラーであった．
* ジェスチャーとして分類された残りの2つの非常に小さなセグメントに対応する16のフレームがあった．
  * この時，widnowサイズは14フレームであった．

---

### セグメントによる分析
* ジェスチャの専門家によれば，セグメントによるエラーの分析を通して，分類器の性能を解析する事ができる．
* セグメントの分類は，セグメント内の正しく分類されたフレーム数によって決定される．
* しかし，フレーム数のしきい値の定義は，専門家によって決定される必要がある．
  * 通常は50%以上のフレームが正しく分類された場合，正しく分割されたと判断する．

---

### 今回は以下の定義を行った．
* (a)内部エラーがセグメント内フレーム数の20%以下なら，正しく分割できたとする．
* (b)誤分類されたセグメント内フレーム数が40%以下なら，正しく分割できたとする．
  * (b)は遷移エラーが無関係と見なす可能性を少なくする事ができる．

---

#### セグメントによる分析
<img src="./image/paper01/table9.png" width="500">

---

#### ジェスチャーフェーズのセグメントによる分析
<img src="./image/paper01/table10.png" width="300">

---

* 3.3.1のジェスチャーフェーズ分割において，誤分類された19のセグメントは1秒未満の間隔であった．
* 同じ事が，観点1で誤分類された27個のジェスチャーフェーズにも生じている．
* 観点2の3つのG-unit分割に関しても同様である．
* これらのエラーは遷移エラーの影響を強く受けている．

---

次スライドに観点3におけるセグメント別の詳細を示す．

---

#### 観点3におけるフェーズ分割のエラー
<img src="./image/paper01/table11.png" width="500">

---

* 4つのセグメントが15フレーム以下であり誤分類されていた．
* これは，前後のフェーズに属していると判定されている．
* さらに、ストロークに相当する34フレームのセグメントは、10個の最初のフレームが正しく分類された．
* 分類器は次のレストポジションに続く後退の次の段階であるストロークを予測出来た事になる．
* ストロークセグメントでは，15フレームが後退として分類され，9フレームがレストポジションに分類された．

---

## 目次
1. Abstract
1. Introduction
1. 理論的背景
1. ジェスチャ分割の自動化
1. 人間の振る舞いへの依存
1. An alternative analysis
1. <font color="Red">Conclution</font>

---

### Conclution
* 本研究ではGユニットとジェスチャーフェーズ分割のためのアプローチを概説した．
* これを分類問題として扱い，SVMを採用した．
* Gユニットの分割に関して
  * 最良の結果として,条件の複雑度に応じて$0.712〜0.910$まで変化するスコアを達成した．
* ジェスチャーフェーズ分割に関しては，いくつかのSVMモデルを定義した．
* いずれの場合もレストポジションとストークの分類に関して有望な成果を残せた．

---

### ジェスチャー分析領域に対する貢献
1. 人間の行動に依存するデータによる複雑さを分析した事
1. ジェスチャー分析の専門家による結果を使って分析をした事

---

### 人間の行動に依存するデータによる複雑さ
* 分類器は人の振る舞いに強い影響を受ける事が確認できた．
* ただし，ビデオ依存のタスクに関しては非常に良い結果を上げる事ができた．
* これは，アナリストはビデオの前半を手動でラベル付すれば，残りのフレームのラベル付を自動化に行う事を可能にしたからである．
* $30\%$をSVMの訓練に使用し，残りをテストに使用したサブ実験を行った．
  * 結果は精度が$0.812$，リコール$0.907$，F値が$0.857$であった．

---

### セグメントによる分析
* ビデオ依存のタスクにおける最良の分類器が良い結果を残した．
* ジェスチャーフェーズ分割において，ビデオ依存タスクのエラー率が31%を示た．
* このエラーの内67%が遷移フレームに対応してた．
* 提唱モデルと専門家の間の違いの多くが遷移フレームで発生すること示している．

---

### 活用場面と可能性
セグメント別の分析結果とビデオ依存タスクの結果は，

統合的ジェスチャーフェーズ分割に特化したシステムと，

専門家の意思決定をサポートするモジュールを備えた,

注釈ツールの実装を支援できる可能性を示せた．

---

### ジェスチャフェーズ分割自動化の利点
* アノテーションプロセスの効率と精度を高める可能性がある．
* 手動注釈には疲労とフレーム内での動きの視覚分析による不正確さがある．
* 自動化できれば，この両方によって引き起こされる主観的な影響を低減できる．

---

### 問題点1
* 今回はwindowによる時間表現アプローチを採用した．
* この仕組はビデオの最初と最後のフレームのラベルを考慮できない．
* また，最良モデルはラベルを中央に配置していた．
* これは，前後のフレームが分類に必要不可欠な事を意味している．
  * リアルタイムでの使用は遅延が発生する問題点がある．

---

### 問題点2
* 加えてこのアプローチは，準備，ホールド，後退に分割する関しての限界を提示した．
* この限界は，フェーズを速度を利用して分割するアプローチに起因する．
* 手動の場合は視覚情報を利用する．
* 視覚による分析では，時間間隔からホールドとレストポジションを判別する事も可能
* 階層的アプローチは最後に分類するフェーズのエラーが多発する事による，全体エラー率の増加

---

### 改善すべき点
* データ表現の改良
  * 手の位置や手首の角速度
  * tension hand information
* その他のアプローチ
  * 他クラス分類問題のアルゴリズムの導入
  * ジェスチャーの構造を決める規則を定義

---

### 議論すべき要素
* 個別に手の動きを分析する事の妥当性
* 各手の対話の内容との関連，
* インタビュー，ディベート，レクチャーなど異なる状況での分析

---

Thanks!!

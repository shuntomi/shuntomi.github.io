## 去年やってた事まとめ
#### 2018 6/30 富澤駿

---

### 今日の内容

1. データ分析のこと
2. 機械学習のこと
3. データのこと
4. 前処理のこと
5. 評価のこと
6. アルゴリズムのこと

---

### あいさつ

岡田研に配属されました富澤です。
岡田研究室のみなさんこれから約二年間よろしくお願いします。

---

## このスライドについて
昨年の後半あたりから、データ分析に関する事を少し勉強していました。  
忘れないようにその概要をまとめておきます。
大した内容ではないですが、これから勉強する人の役に立てば幸いです。

##### 概要なので基本的に具体的な手法の話は省いています。


---

## 1. データ分析のこと

---

### データ分析ブーム

ここ数年、ビックデータ、機械学習、DeepLearningブームのおかげで、    

データ分析の需要が増えている(らしい)。

なので少し勉強してみようと思いました。

---

### データ分析の流れ

1. 目的設定
2. 分析計画
3. データ設計
4. データ収集
5. データの前処理
6. 分析手法の選択と適用
7. 分析結果の解釈
8. 施策の提案
9. 実施と検証

---

### データ分析の流れ

1. 目的設定
2. 分析計画
3. データ設計
4. データ収集
5. <font color='firebrick'>データの前処理 </font>
6. <font color='firebrick'>分析手法の選択と適用 </font>
7. <font color='firebrick'>分析結果の解釈 </font>
8. 施策の提案
9. 実施と検証

このスライドでは5~7の概要を説明します。

---

### データ分析に使われる知識

- 統計学、機械学習、数理最適化の知識
- プログラミングの知識

---

### よく使われる言語

- SQL
- python
  - Numpy,pandas,matplotlib
  - sklearn,Keras,Tensor-flow
- R(僕はRをほとんど触った事がないので分かりません...)
  - dplyr
  - packages
  - ggplot

---

## 2. 機械学習のこと

---

### 機械学習とは

wikipediaよりコピペ
><font color='firebrick'> 機械学習（Machine Learning）</font>とは、人工知能における研究課題の一つで、人間が自然に行っている学習能力と同様の機能をコンピュータで実現しようとする技術・手法のことである。

---

### 応用分野

機械学習は多くの分野で応用されています。
- 文字認識
- 画像解析
- 音声解析
- 自然言語処理
- ロボット
- 市場予測
- 医療診断

---

### 機械学習の分類

機械学習の手法は大きく２つに分けられます。  
- 教師あり学習
  - 回帰
  - 分類
- 教師なし学習
  - クラスタリング
  - 次元削減
  - 異常検知

---

### その他の分類

他にもいろいろあるみたいです。
- アンサンブル学習
- 半教師あり学習
- 強化学習(日本語本が少ない。名前カッコいい)
- 転移学習
- 順序学習
- オンライン学習

---

## 教師あり学習

---

### 教師あり学習とは

<font color='firebrick'>教師あり学習（Supervised Learning）</font>とは  
入力と出力の組からなるデータをもとに学習を行う方法です。  

---

### 教師あり学習の分類

教師あり学習は回帰問題と分類問題に分類されます。  
- 回帰問題
  - 出力が連続値
  - 関数近似みたいなやつ
- 分類問題
  - 出力が離散値
  - その名の通りグループ分けする

---

### 回帰

<font color='firebrick'>回帰（regression）</font>とはデータに良くあてはまる曲面を見つける分析方法です。
<img src="./figures/01/001.png"></img>

---

### 回帰分析の目的

回帰分析は主に２つの目的で利用されます。
- 『結果』に対する『原因』を推測すること
- 『過去』から『未来』を予測すること

---

### 単回帰

例えば単回帰という方法があります。  
データに直線を当てはめる方法です。  
<img src="./figures/01/002.png"></img>  

---

### 上手くいかない例

ただし、単回帰ではうまくいかない場合もあります。  
<img src="./figures/01/004.png"></img>


---

### 別の回帰手法

左は重回帰、右は決定木回帰の例です。  
<img src="./figures/01/006.png"></img>
<img src="./figures/01/005.png"></img>  
他にも様々な回帰手法が知られています。

---

### 回帰ができると...

株価の予測とかもできそうです。  
できたらウハウハですね。

<img src="./figures/01/200.png", width='400px'></img>

---

### 分類

<font color='firebrick'>分類（classification）</font>とはデータを隔てる境界線を見つける分析方法です。  
大きく２クラス問題と多クラス問題に分けられます。
<img src="./figures/01/007.png", width='400px'></img>

---

### ２クラス分類

SVMを用いた２クラス分類の例です。  
<img src="./figures/01/008.png", width='400px'></img>

---

### 他クラス分類

他クラスのデータを分類する問題もあります。  
<img src="./figures/01/009.png", width='400px'></img>

---

### 別の分類手法

左から決定木、N近傍法、多クラスSVMの例です。  
<img src="./figures/01/010.png", width='250px'></img>
<img src="./figures/01/011.png", width='250px'></img>
<img src="./figures/01/012.png", width='250px'></img>

---

### 分類ができると...

手書き文字や物体の認識もできそうです。  

<img src="./figures/01/mnist.png"></img>


---

### 教師あり学習の手法

他にもたくさんの手法が知られています。  

- 線形モデル
- ロジスティック回帰
- 判別分析
- k近傍法
- 決定木
- SVM
- ニューラルネットワーク
- ランダムフォレスト

---

## 教師なし学習

---

### 教師なし学習とは

<font color='firebrick'>教師なし学習（Unsupervised Learning）</font>とは  
入力と出力の組からなるデータをもとに学習を行う方法です。  

---

### 教師なし学習の分類

教師なし学習はクラスタリングと次元削減に分類されます。  
- クラスタリング
  - グループわけするやつ
- 次元削減
  - へらすやつ

---

### クラスタリング

<font color='firebrick'>クラスタリング（Clustering）</font>とは  
データにグループを割り当てる手法です。  
<img src="./figures/01/015.png", width='250px'></img>

---

### クラスタリングの例

左がK-means、右がDBSCANの例です。  
<img src="./figures/01/017.png", width='250px'></img>
<img src="./figures/01/016.png", width='250px'></img>

---

### 次元削減

<font color='firebrick'> 次元削減（Dimension Reduction）</font>とは  
高次元のデータを低次元のデータに変換する方法です。  
<img src="./figures/01/013.png", width='250px'></img>

---

### 次元削減の例

左がLLE、右がtSNEの例です。  
<img src="./figures/01/014.png", width='250px'></img>
<img src="./figures/01/018.png", width='250px'></img>

---

## 教師なし学習の手法

他にもたくさんの手法が知られています。

- K-means
- 階層的クラスタリング
- トピックモデル
- 主成分分析
- 因子分析
- 独立成分分析

---

### ノーフリーランチの定理

『あらゆる問題で性能の良い  
万能な学習アルゴリズムは存在しない。』

なので、目的に適したアルゴリズムを選択しましょう。

---

## データのこと

---

データ分析をはじめるには、まずデータが必要です。  
データの種類や扱い方を少しだけ紹介しようと思います。  

---

よくあるデータ形式としてこんな感じのやつがあります。  


| 年齢 | 性別 | 地区 | 登録日 | 解約 |
|:-----------|------------:|:------------:|:------------:|:------------:|
| 31 | 男 | 東京 | 2003/04/12 | 1 |
| 45 | 男 | 神奈川 | 2012/09/21 | 0 |
| 23 | 女 | 埼玉 | 2009/12/01 | 0 |
| 28 | 男 | 東京 | 2011/11/04 | 1 |
| 34 | 女 | 大阪 | 2008/03/21 | 0 |
| 19 | 女 | 北海道 | 2003/08/12 | 0 |

---

### データの種類

- 名義尺度
	- 名前、電話番号など
- 順序尺度
	- レースの着順
- 間隔尺度
	- 温度（乗除不可）
- 比例尺度
	- 質量、長さなど

---

### データの分類

- 数値データ（量的変数）
	- 比例尺度
	- 間隔尺度
- カテゴリデータ（質的変数）
	- 名義尺度
	- 順序尺度

---

機械学習のアルゴリズムは  
数値データを前提としているものが大半です。

---

### ダミー変数化

カテゴリデータを数値データに変換するテクニックがあります。  
1列目から2,3列目への変換はダミー変数化と呼ばれます。

| 性別 | 男 | 女  |
|:-----------|:------------:|:-----------|
| 男 | 1 | 0 |
| 男 | 1 | 0 |
| 女 | 0 | 1 |
| 男 | 1 | 0 |
| 女 | 0 | 1 |
| 女 | 0 | 1 |

---

### データの欠損

データにはしばしば以下のような欠損値を含む場合があります。  

| 年齢 | 性別 | 地区 | 登録日 | 解約 |
|:-----------|------------:|:------------:|:------------:|:------------:|
| 31 | 男 | 東京 | 2003/04/12 | 1 |
| 45 | 男 | 神奈川 | 2012/09/21 | 0 |
|  | 女 | 埼玉 | 2009/12/01 | 0 |
| 28 | 男 | 東京 | 2011/11/04 | 1 |
| 34 |  | 大阪 |  | 0 |
| 19 | 女 | 北海道 | 2003/08/12 | 0 |

---

### 欠損値の扱い方

欠損値は適当に処理しておく必要があります。
- 捨てる
	- 欠損値が少量、データが大量
- 置換する
	- 代表値
	- 回帰

---

### 標準化

必要であれば特徴量ごとに<font color='firebrick'>データの標準化（Normalization）</font>を行います。  
標準化により平均$0$分散$1$へ変換できます。

\begin{eqnarray}
z = \frac{x - \mu}{\sigma}
\end{eqnarray}

$\mu$は$x$の平均、$\sigma$は$x$の標準偏差です。


---

### 特徴選択

特徴の中から有用なものを選び出す

- ステップワイズ法(まだ勉強してないです)
  - 前向き法、後ろ向き法、ハイブリット法
- 機械学習による選択
  - 決定木、ランダムフォレスト、ラッソ回帰
- 次元削減
  - 主成分分析、判別分析

---

### 醜いアヒルの子定理

醜いアヒルの子と普通のアヒルの子の類似性は  
２羽の普通のアヒルの子の類似性と等しい。

ようするに、万能な特徴量は存在しない。  
目的に適した特徴量を使えという事らしいです。  

---

## 評価のこと

---

### 回帰モデルの評価基準

- 平均絶対誤差
  - $\mathrm{MAE} = \frac{1}{N}(\sum\_{i=1}^{N} |y\_i - \hat{y\_i}|)$
- 平均二乗誤差
  - $\mathrm{MSE} = \frac{1}{N}(\sum\_{i=1}^{N}(y\_i - \hat{y\_i})^2)$
- 平均絶対パーセント誤差
  - $\mathrm{MAPE} = \frac{1}{N}|(y\_t - \hat{y}\_t)/y\_t|$

---

### 分類モデルの評価基準

- 精度
  - 正解数/データ数
- 誤差率
  - 1 - 精度
- 混同行列(ROCカーブ)
  - 適合率
  - 再現率
  - F値

---

## アルゴリズムのこと

---

機械学習のアルゴリズムの一例としてK-means法の  
アルゴリズムを紹介しようと思います。

#### 多分岡田先生の講義でも紹介がある気がします。

---

K-means法はクラスタリング手法の一つで、  
おそらく最も有名なクラスタリングアルゴリズムです。  

---


$D$次元ユークリッド空間上の$N$個の観測点で構成される  
データ集合$\{ \mathbf{x}\_1,\dots,\mathbf{x}\_N \}$が存在する場合を考えます。  
<br>
K-means法では、データ集合を$K$個のクラスタに分割することを目的とします。  
(※  とりあえず今回は$K$の値は人が与える事にしときます。)

---

直感的にクラスタとは、その内部のデータ点間の距離が、  
外部のデータとの距離と比べて小さいデータのグループのことです。
<img src="./figures/01/017.png"></img>

---


これは代表ベクトルと呼ばれる$K$個の$D$次元ベクトル$\mathbf{\mu}\_k \;\; (k=1,\dots,K)$を  
導入することで定式化することができます。($\mathbf{\mu}\_k$は$K$番目のクラスタの代表ベクトルです)  
<br>
先に言っておくと、$\mathbf{\mu}\_k$は$K$番目のクラスタの中心とみなすことができます。


---


K-means法では、ベクトルの集合$\{ \mathbf{\mu}\_k \}$をうまく決定し、  
全データ点をうまく各クラスタに対応させることで、  
各データ点から対応する$\mathbf{\mu}\_k$への二乗平均の総和の最小化を目指します。

---


ここで、データ点のクラスタへの割当を表す記法を定義しておくと便利です。  
データ$\mathbf{x}\_n$がクラスタ$k$に帰属するか否かを表す変数$r\_{nk}$を以下の様に定義します。

\begin{eqnarray}
r\_{nk} = \begin{cases}
        1 & (\mathbf{x}\_nがクラスタkに属する場合) \\\\
        0 & (それ以外の場合)
        \end{cases}
\end{eqnarray}

---

そして、K-means法の目的関数$J$を次の様に定義します。

\begin{equation}
J = \sum\_{n=1}^{N} \sum\_{k=1}^{K} r\_{nk} \|\| \mathbf{x}\_n - \mathbf{\mu}\_k \|\|^2
\end{equation}

これは各データ点からそれらが割り当てられたベクトル$\mathbf{\mu}\_k$までの  
二乗和の総和を表しており、歪み尺度と呼ばれることもあります。  

K-means法の目的は、$J$を最小にする$\{ r\_{nk} \}$と$\{ \mathbf{\mu}\_k \}$を求めることです。


---


$J$の最小化は$\mathbf{\mu}\_k$を初期化した後に、次の2つのステップを繰り返すことで実現できます。
<p></p>
<div style="padding: 20px; margin-bottom: 10px; border: 2px solid #333333;">

<table align="center">
<tr><td>
【ステップ1】: $\mathbf{\mu}\_k$を固定しつつ$r\_{nk}$について$J$を最小化する。<br>
【ステップ2】: $r\_{nk}$を固定しつつ$\mathbf{\mu}\_{nk}$について$J$を最小化する。<br>
</td></tr>
</table>
</div>

---


まず、$r\_{nk}$について$J$を最小化することを考えます。  
$J = \sum\_{n=1}^{N} \sum\_{k=1}^{K} r\_{nk} \|\| \mathbf{x}\_n - \mathbf{\mu}\_k \|\|^2$より、$J$は$r\_{nk}$について線形なので、  
最適化は代数的に解くことが可能です。

異なる$n$を含む項は互いに独立なので、各$n$について別々に、$\|\| \mathbf{x}\_n - \mathbf{\mu}\_k \|\|^2$が  
最小になるような$k$の値に対して$r\_{nk}=1$とすれば良いです。  

つまり、次式のように$r\_{nk}$を決めれば良いです。
\begin{eqnarray}
r\_{nk} = \begin{cases}
        1 & k=\mathrm{arg} \min\_j \|\| \mathbf{x}\_n - \mathbf{\mu}\_j \|\|^2のとき \\\\
        0 & (それ以外の場合)
        \end{cases}
\end{eqnarray}

---


次に$\mathbf{\mu}\_k$について$J$を最小化することを考えます。  
$J = \sum\_{n=1}^{N} \sum\_{k=1}^{K} r\_{nk} \|\| \mathbf{x}\_n - \mathbf{\mu}\_k \|\|^2$より、$J$は$\mathbf{\mu}\_k$の二次関数なので、  
次のように$\mathbf{\mu}\_k$に関する偏微分を$0$と置くことで最小化できます。
\begin{equation}
2 \sum\_{n=1}^N r\_{nk}(\mathbf{x}\_n - \mathbf{\mu}\_k) = 0
\end{equation}


---

$\mathbf{\mu}\_k$について解くと、次式を得ます。  
\begin{equation}
\mathbf{\mu}\_k = \frac{\sum\_n r\_{nk} \mathbf{x}\_n}{\sum\_n r\_{nk}}
\end{equation}
<br>
この式は、$\mathbf{\mu}\_k$を$k$番目のクラスタに割り当てられたすべてのデータ点$\mathbf{x}\_n$の  
平均値とおいていると解釈することができます。(これがK-means法の名の由来です)

---

K-means法のアルゴリズムをまとめると次のようになります。
<p></p>
<div style="padding: 20px; margin-bottom: 10px; border: 2px solid #333333;">

<table align="center">
<tr><td>
1. $\mathbf{\mu}\_k$の初期値を選ぶ<br>
<p></p>
2.【ステップ1】以下の式で$r\_{nk}$を計算。<br>
\begin{eqnarray}
r\_{nk} = \begin{cases}
        1 & k=\mathrm{arg} \min\_j \|\| \mathbf{x}\_n - \mathbf{\mu}\_j \|\|^2のとき \\\\
        0 & (それ以外の場合)
        \end{cases}
\end{eqnarray}
<p></p>
3.【ステップ2】求めた$r\_{nk}$で$\mathbf{\mu}\_{nk}$を再計算。<br>
\begin{equation}
\mathbf{\mu}\_k = \frac{\sum\_n r\_{nk} \mathbf{x}\_n}{\sum\_n r\_{nk}}
\end{equation}

4. 収束条件が満たされていなければ、ステップ2に戻る。
</td></tr>
</table>
</div>

上記のように2つのステップを収束するまで（もしくはあらかじめ定めた  
最大繰り返し数を超えるまで）繰り返します。  
<a href="http://tech.nitoyon.com/ja/blog/2013/11/07/k-means/" target="blank">K-meansのアニメーション</a>

---

## その他のこと

---

他にも必要な知識はたくさんあります。
僕も勉強不足で全てを理解しているわけではないので、
知ってる人は教えてくれたら嬉しいです。

- 過学習、汎化性能、正則化
- パラメータチューニング(Adam,Adgrad,...)
- データの分け方(ホールドアウト、交差検定、ブートストラップ)

---

以上です。ありがとうございました。

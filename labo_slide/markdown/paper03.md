# Automatic Annotation of Co-speech Gestures based on Wrist Movement and Rest Positions
### 2019/7/18 富澤駿

---

### 概要
* 通常のRGBカメラを使用し，会話と共起するジェスチャーの自動検出を行う
    * 動画データがあれば適応かのうであり，導入しやすい
* 特量量として「手の動き」と「手とレストポジションの距離」を使用する
* 検出タスクではF値が94.98%であった

---

### 自分の研究との差異
* ジェスチャー検出が目的(自分の研究は機能ラベルの推定)
* 各ジェスチャーは自然に発生させた物ではなく，意図的に発生させた物を使用している
* 体の部位の座標はRGBカメラとOpen Poseで取得している
* 機械学習アルゴリズムは使用していない
* 被験者が一人なので個人特性の影響を考慮していない．

---

### 検出の大まかな流れ
1. RGBカメラとOpenPoseを使用し，各部位の二次元座標を取得
1. 手とレストポジションの距離を$c\_{d}(t)$として定義
1. パラメータによってwindowを定義し，windos内の両手の標準偏差の大きい方をフレーム$t$の運動量コスト$c\_{m}(t)$として定義．
1. $c\_{d}(t)$，$c\_{m}(t)$の重み付き平均$c(t)$を計算
1. しきい値を設定し，$c(t)$の値によってジェスチャーかどうか判定

---

### レストポジションとハンドポジション
<img src="./image/paper03/position.png" width="500"></img>

---

### 実験　
* negation,palm up,pointing,me,precision gropの5種類のジェスチャーから構成されるデータセット
* 話しながら5つのジェスチャーの取得を行った．
* ジェスチャーは意図的に発生させている．ただし動きの指定まではしない．
* 被験者は一人

---

### 結果 混合行列
\begin{array}{c|c|c}
   & Predicted Rest & Predicted Gesture \\\
   \\hline
  Actual Rest   & 6516   & 242 \\\
  Actual Gesture   & 329 & 5400 \\\
\end{array}

\begin{equation}
 P=95.71,F\_1=\boldsymbol{94.98}
\end{equation}


---

### 結果 各ジェスチャに関しての精度
<img src="./image/paper03/result.png" width="500"></img>

---

### 結論
* F値が94.98%を記録した
* 実験で使われたデータは同じ話者である．
* 今後は被験者や設定を変えて実験を行う必要がある．
* このときレストとレスト付近のホールドの区別が付きづらい可能性がある
* 場合によっては手の形など新たな特徴量を使用する必要がある．
